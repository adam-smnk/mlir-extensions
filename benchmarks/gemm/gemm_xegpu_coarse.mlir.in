// RUN: %python_executable %imex_runner --requires=l0-runtime -i %s --pass-pipeline-file=%p/xegpu-to-llvm.pp \
// RUN:                                       --runner imex-cpu-runner -e main \
// RUN:                                       --entry-point-result=void \
// RUN:                                       --shared-libs=%irunner_utils,%mlir_runner_utils,%mlir_c_runner_utils,%levelzero_runtime --filecheck
// RUN: %python_executable %imex_runner --requires=sycl-runtime -i %s --pass-pipeline-file=%p/xegpu-to-llvm.pp \
// RUN:                                        --runner imex-cpu-runner -e main \
// RUN:                                        --entry-point-result=void \
// RUN:                                        --shared-libs=%irunner_utils,%mlir_runner_utils,%mlir_c_runner_utils,%sycl_runtime --filecheck
#mapRow = affine_map<(d0) -> (d0 * 64)>
#mapCol = affine_map<(d0) -> (d0 * 128)>

module @gemm attributes {gpu.container_module} {
  func.func @test(%A: memref<@MB@x@SIZE@xf16>, %B: memref<@SIZE@x@SIZE@xf16>, %C: memref<@MB@x@SIZE@xf32>) -> memref<@MB@x@SIZE@xf32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c8 = arith.constant 8 : index
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %A_gpu = gpu.alloc  host_shared () : memref<@MB@x@SIZE@xf16>
    memref.copy %A, %A_gpu : memref<@MB@x@SIZE@xf16> to memref<@MB@x@SIZE@xf16>
    %B_gpu = gpu.alloc  host_shared () : memref<@SIZE@x@SIZE@xf16>
    memref.copy %B, %B_gpu : memref<@SIZE@x@SIZE@xf16> to memref<@SIZE@x@SIZE@xf16>
    %C_gpu = gpu.alloc  host_shared () : memref<@MB@x@SIZE@xf32>
    memref.copy %C, %C_gpu : memref<@MB@x@SIZE@xf32> to memref<@MB@x@SIZE@xf32>

    %dimM = memref.dim %C, %c0 : memref<@MB@x@SIZE@xf32>
    %dimN = memref.dim %C, %c1 : memref<@MB@x@SIZE@xf32>

    %tileSizeM = arith.constant 64 : index
    %tileSizeN = arith.constant 128 : index
    %numTilesX = arith.divui %dimM, %tileSizeM : index
    %numTilesY = arith.divui %dimN, %tileSizeN : index

    gpu.launch_func  @test_kernel::@test_kernel blocks in (%numTilesX, %numTilesY, %c1) threads in (%c4, %c4, %c1) args(%A_gpu : memref<@MB@x@SIZE@xf16>, %B_gpu : memref<@SIZE@x@SIZE@xf16>, %C_gpu : memref<@MB@x@SIZE@xf32>)

    // %cast = memref.cast %C_gpu : memref<@MB@x@SIZE@xf32> to memref<*xf32>
    // call @printMemrefF32(%cast) : (memref<*xf32>) -> ()

    gpu.dealloc  %A_gpu : memref<@MB@x@SIZE@xf16>
    gpu.dealloc  %B_gpu : memref<@SIZE@x@SIZE@xf16>
    return %C_gpu : memref<@MB@x@SIZE@xf32>
  }
  func.func private @printMemrefF32(memref<*xf32>) attributes {llvm.emit_c_interface}
  gpu.module @test_kernel attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.4, [Addresses, Float16Buffer, Int64, Int16, Int8, Kernel, Linkage, Vector16, GenericPointer, Groups, Float16, Float64, AtomicFloat32AddEXT, ExpectAssumeKHR, SubgroupDispatch, VectorComputeINTEL, VectorAnyINTEL], [SPV_EXT_shader_atomic_float_add, SPV_KHR_expect_assume, SPV_INTEL_vector_compute]>, api=OpenCL, #spirv.resource_limits<subgroup_size = 16>>} {
    gpu.func @test_kernel(%A: memref<@MB@x@SIZE@xf16>, %B: memref<@SIZE@x@SIZE@xf16>, %C: memref<@MB@x@SIZE@xf32>) kernel attributes {VectorComputeFunctionINTEL, spirv.entry_point_abi = #spirv.entry_point_abi<>} {
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c16 = arith.constant 16 : index
      %c24 = arith.constant 24 : index
      %c32 = arith.constant 32 : index
      %c48 = arith.constant 48 : index
      %c64 = arith.constant 64 : index
      %c128 = arith.constant 128 : index
      %c256 = arith.constant 256 : index

      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %4 = affine.apply #mapRow(%0)
      %5 = affine.apply #mapCol(%1)
      %2 = gpu.thread_id  x
      %3 = gpu.thread_id  y // Contiguous vectorizable dimension.

      // Block tiles.
      // Created by initial GEMM tiling.
      // Each thread block computes one C tile.
      //
      // PARAM: sizes have to match new GEMM tile size
      // %blockA = memref.subview %A[%4, 0] [16, 1024] [1, 1] : memref<@MB@x@SIZE@xf16> to memref<16x@SIZE@xf16, strided<[1024, 1], offset: ?>>
      // %blockB = memref.subview %B[0, %5] [1024, 32] [1, 1] : memref<@SIZE@x@SIZE@xf16> to memref<@SIZE@x32xf16, strided<[1024, 1], offset: ?>>
      // %blockC = memref.subview %C[%4, %5] [16, 32] [1, 1] : memref<@MB@x@SIZE@xf32> to memref<16x32xf32, strided<[1024, 1], offset: ?>>

      // Thread tile sizes.
      // Each thread will compute <2x2> DPAS tile (<8x16> elements) of C tile.
      %TM = arith.constant 16 : index
      %TN = arith.constant 32 : index
      %stepTM = arith.constant 8 : index
      %stepTN = arith.constant 16 : index

      // Block tile sizes.
      // Parallel dimensions are based on the original tiling size.
      // Reduction dimension tiling is chosen to match thread tile sizes.
      //
      // PARAM: block sizes BM and BN have to match GEMM tile size
      %BM = arith.constant 64 : index
      %BN = arith.constant 128 : index
      %BK = arith.constant 32 : index // == %blockDimK - matches inner block tile dim size
      %numThreadTiles = arith.divui %BK, %TN : index

      // Find size of the GEMM tiles reduction dimension.
      %dimK = memref.dim %A, %c1 : memref<@MB@x@SIZE@xf16>
      %numSubTilesK = arith.ceildivsi %dimK, %BK : index

      // Initialize accumulator registers.
      //
      // Each thread loads C tiles it will compute.
      %threadOffsetRow = arith.muli %2, %TM : index
      %threadOffsetCol = arith.muli %3, %TN : index
      %outTileRow = arith.addi %4, %threadOffsetRow : index
      %outTileCol = arith.addi %5, %threadOffsetCol : index
      %otr_0 = arith.addi %outTileRow, %c0 : index
      %otr_1 = arith.addi %outTileRow, %stepTM : index
      %otc_0 = arith.addi %outTileCol, %c0 : index
      %otc_1 = arith.addi %outTileCol, %stepTN : index

      %tileC_0_0 = xegpu.create_nd_tdesc %C[%otr_0, %otc_0] {mode = vc} : memref<@MB@x@SIZE@xf32> -> !xegpu.tensor_desc<8x16xf32>
      %tileC_0_1 = xegpu.create_nd_tdesc %C[%otr_0, %otc_1] {mode = vc} : memref<@MB@x@SIZE@xf32> -> !xegpu.tensor_desc<8x16xf32>
      %tileC_1_0 = xegpu.create_nd_tdesc %C[%otr_1, %otc_0] {mode = vc} : memref<@MB@x@SIZE@xf32> -> !xegpu.tensor_desc<8x16xf32>
      %tileC_1_1 = xegpu.create_nd_tdesc %C[%otr_1, %otc_1] {mode = vc} : memref<@MB@x@SIZE@xf32> -> !xegpu.tensor_desc<8x16xf32>

      %tileA_0_0 = xegpu.create_nd_tdesc %A[%otr_0, %c0] {mode = vc} : memref<@MB@x@SIZE@xf16> -> !xegpu.tensor_desc<8x16xf16>
      %tileA_0_1 = xegpu.create_nd_tdesc %A[%otr_0, %c0] {mode = vc} : memref<@MB@x@SIZE@xf16> -> !xegpu.tensor_desc<8x16xf16>
      %tileA_1_0 = xegpu.create_nd_tdesc %A[%otr_1, %c0] {mode = vc} : memref<@MB@x@SIZE@xf16> -> !xegpu.tensor_desc<8x16xf16>
      %tileA_1_1 = xegpu.create_nd_tdesc %A[%otr_1, %c0] {mode = vc} : memref<@MB@x@SIZE@xf16> -> !xegpu.tensor_desc<8x16xf16>

      %tileB_0_0 = xegpu.create_nd_tdesc %B[%c0, %otc_0] {mode = vc} : memref<@SIZE@x@SIZE@xf16> -> !xegpu.tensor_desc<16x16xf16>
      %tileB_0_1 = xegpu.create_nd_tdesc %B[%c0, %otc_1] {mode = vc} : memref<@SIZE@x@SIZE@xf16> -> !xegpu.tensor_desc<16x16xf16>
      %tileB_1_0 = xegpu.create_nd_tdesc %B[%c0, %otc_0] {mode = vc} : memref<@SIZE@x@SIZE@xf16> -> !xegpu.tensor_desc<16x16xf16>
      %tileB_1_1 = xegpu.create_nd_tdesc %B[%c0, %otc_1] {mode = vc} : memref<@SIZE@x@SIZE@xf16> -> !xegpu.tensor_desc<16x16xf16>

      %vC_0_0 = xegpu.load_nd %tileC_0_0 {mode = vc} : !xegpu.tensor_desc<8x16xf32> -> vector<8x16xf32>
      %vC_0_1 = xegpu.load_nd %tileC_0_1 {mode = vc} : !xegpu.tensor_desc<8x16xf32> -> vector<8x16xf32>
      %vC_1_0 = xegpu.load_nd %tileC_1_0 {mode = vc} : !xegpu.tensor_desc<8x16xf32> -> vector<8x16xf32>
      %vC_1_1 = xegpu.load_nd %tileC_1_1 {mode = vc} : !xegpu.tensor_desc<8x16xf32> -> vector<8x16xf32>

      xegpu.compile_hint

      %out:12 = scf.for %subtileIv = %c0 to %numSubTilesK step %c1 iter_args(
        %tA_0_0 = %tileA_0_0,
        %tA_0_1 = %tileA_0_1,
        %tA_1_0 = %tileA_1_0,
        %tA_1_1 = %tileA_1_1,

        %tB_0_0 = %tileB_0_0,
        %tB_0_1 = %tileB_0_1,
        %tB_1_0 = %tileB_1_0,
        %tB_1_1 = %tileB_1_1,

        %acc_0_0 = %vC_0_0,
        %acc_0_1 = %vC_0_1,
        %acc_1_0 = %vC_1_0,
        %acc_1_1 = %vC_1_1
      ) -> (
        !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>,
        !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>,
        vector<8x16xf32>, vector<8x16xf32>, vector<8x16xf32>, vector<8x16xf32>
      ) {
        // Load sub-tiles of A and B tiles from GMEM to SMEM.
        // The sub-tiles are loaded cooperatively using all threads in a threadblock.
        // Find the start position of a sub-tile.
        %subtileOffset = arith.muli %subtileIv, %BK : index

        // %subA = memref.subview %blockA[0, %subtileOffset] [16, 32] [1, 1] : memref<16x@SIZE@xf16, strided<[1024, 1], offset: ?>> to memref<16x32xf16, strided<[1024, 1], offset: ?>>
        // %subB = memref.subview %blockB[%subtileOffset, 0] [32, 32] [1, 1] : memref<@SIZE@x32xf16, strided<[1024, 1], offset: ?>> to memref<32x32xf16, strided<[1024, 1], offset: ?>>

        // Fetch data from GMEM to SMEM using all threads in a threadblock.
        // Each thread has to load 1 tile of A and B from their block tiles.
        // %tColA = arith.addi %subtileOffset, %threadOffsetCol : index
        // %tRowB = arith.addi %subtileOffset, %threadOffsetRow : index
        // %tileA = xegpu.create_nd_tdesc %A[%outTileRow, %tColA] {mode = vc} : memref<@MB@x@SIZE@xf16> -> !xegpu.tensor_desc<16x32xf16>
        // %tileB = xegpu.create_nd_tdesc %B[%tRowB, %outTileCol] {mode = vc} : memref<@SIZE@x@SIZE@xf16> -> !xegpu.tensor_desc<32x32xf16>

        // Use prefetching to cache all the A and B sub-tiles.
        // They will be shared among threads within the block.
        // xegpu.prefetch_nd %tileA {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<16x32xf16>
        // xegpu.prefetch_nd %tileB {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<32x32xf16>

        // xegpu.compile_hint

        // Synchronize all threads in a threadblock.
        // Whole A and B sub-tiles are needed to perform computation.
        // Wait for all threads in a threadblock to finish loading A and B tile elements.
        // TOOD: see if needed, HW might enforce cache coherency on its own
        // gpu.barrier

        // GEMM computation.
        %vA_0_0 = xegpu.load_nd %tA_0_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 1}: !xegpu.tensor_desc<8x16xf16> -> vector<8x8x2xf16>
        %vA_0_1 = xegpu.load_nd %tA_0_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 1}: !xegpu.tensor_desc<8x16xf16> -> vector<8x8x2xf16>
        %vA_1_0 = xegpu.load_nd %tA_1_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 1}: !xegpu.tensor_desc<8x16xf16> -> vector<8x8x2xf16>
        %vA_1_1 = xegpu.load_nd %tA_1_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 1}: !xegpu.tensor_desc<8x16xf16> -> vector<8x8x2xf16>

        %vB_0_0 = xegpu.load_nd %tB_0_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 0} : !xegpu.tensor_desc<16x16xf16> -> vector<8x16x2xf16>
        %vB_0_1 = xegpu.load_nd %tB_0_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 0} : !xegpu.tensor_desc<16x16xf16> -> vector<8x16x2xf16>
        %vB_1_0 = xegpu.load_nd %tB_1_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 0} : !xegpu.tensor_desc<16x16xf16> -> vector<8x16x2xf16>
        %vB_1_1 = xegpu.load_nd %tB_1_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached, vnni_axis = 0} : !xegpu.tensor_desc<16x16xf16> -> vector<8x16x2xf16>

        %nextTileA_0_0 = xegpu.update_nd_offset %tA_0_0, [%c0, %TN] {mode = vc}: !xegpu.tensor_desc<8x16xf16> -> !xegpu.tensor_desc<8x16xf16>
        %nextTileA_0_1 = xegpu.update_nd_offset %tA_0_1, [%c0, %TN] {mode = vc}: !xegpu.tensor_desc<8x16xf16> -> !xegpu.tensor_desc<8x16xf16>
        %nextTileA_1_0 = xegpu.update_nd_offset %tA_1_0, [%c0, %TN] {mode = vc}: !xegpu.tensor_desc<8x16xf16> -> !xegpu.tensor_desc<8x16xf16>
        %nextTileA_1_1 = xegpu.update_nd_offset %tA_1_1, [%c0, %TN] {mode = vc}: !xegpu.tensor_desc<8x16xf16> -> !xegpu.tensor_desc<8x16xf16>

        %nextTileB_0_0 = xegpu.update_nd_offset %tB_0_0, [%TN, %c0] {mode = vc}: !xegpu.tensor_desc<16x16xf16> -> !xegpu.tensor_desc<16x16xf16>
        %nextTileB_0_1 = xegpu.update_nd_offset %tB_0_1, [%TN, %c0] {mode = vc}: !xegpu.tensor_desc<16x16xf16> -> !xegpu.tensor_desc<16x16xf16>
        %nextTileB_1_0 = xegpu.update_nd_offset %tB_1_0, [%TN, %c0] {mode = vc}: !xegpu.tensor_desc<16x16xf16> -> !xegpu.tensor_desc<16x16xf16>
        %nextTileB_1_1 = xegpu.update_nd_offset %tB_1_1, [%TN, %c0] {mode = vc}: !xegpu.tensor_desc<16x16xf16> -> !xegpu.tensor_desc<16x16xf16>

        xegpu.prefetch_nd %nextTileA_0_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x16xf16>
        xegpu.prefetch_nd %nextTileA_0_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x16xf16>
        xegpu.prefetch_nd %nextTileA_1_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x16xf16>
        xegpu.prefetch_nd %nextTileA_1_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x16xf16>

        xegpu.prefetch_nd %nextTileB_0_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<16x16xf16>
        xegpu.prefetch_nd %nextTileB_0_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<16x16xf16>
        xegpu.prefetch_nd %nextTileB_1_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<16x16xf16>
        xegpu.prefetch_nd %nextTileB_1_1 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<16x16xf16>

        xegpu.compile_hint

        // GEMM computation
        %dpas_0_0_temp = xegpu.dpas %vA_0_0, %vB_0_0, %acc_0_0 {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_0_1_temp = xegpu.dpas %vA_0_0, %vB_0_1, %acc_0_1 {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_1_0_temp = xegpu.dpas %vA_1_0, %vB_0_0, %acc_1_0 {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_1_1_temp = xegpu.dpas %vA_1_0, %vB_0_1, %acc_1_1 {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>

        %dpas_0_0 = xegpu.dpas %vA_0_1, %vB_1_0, %dpas_0_0_temp {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_0_1 = xegpu.dpas %vA_0_1, %vB_1_1, %dpas_0_1_temp {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_1_0 = xegpu.dpas %vA_1_1, %vB_1_0, %dpas_1_0_temp {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>
        %dpas_1_1 = xegpu.dpas %vA_1_1, %vB_1_1, %dpas_1_1_temp {mode = vc} : vector<8x8x2xf16>, vector<8x16x2xf16>, vector<8x16xf32> -> vector<8x16xf32>

        xegpu.compile_hint
        // Synchronize all threads in a threadblock.
        // All current computations have to be finished before SMEM A and B tiles can be
        // replaced with new values (new tiles) from GMEM.
        // TODO: see if needed, cache might be large enough to allow prefetching of the next set of tiles
        gpu.barrier

        scf.yield
          %nextTileA_0_0,
          %nextTileA_0_1,
          %nextTileA_1_0,
          %nextTileA_1_1,
          %nextTileB_0_0,
          %nextTileB_0_1,
          %nextTileB_1_0,
          %nextTileB_1_1,
          %dpas_0_0,
          %dpas_0_1,
          %dpas_1_0,
          %dpas_1_1
          : !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>, !xegpu.tensor_desc<8x16xf16>,
          !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>, !xegpu.tensor_desc<16x16xf16>,
          vector<8x16xf32>, vector<8x16xf32>, vector<8x16xf32>, vector<8x16xf32>
      }

      // Store the final C tile element values.
      xegpu.store_nd %out#8, %tileC_0_0 {mode = vc, l1_hint = write_back, l2_hint = write_back, l3_hint = write_back} : vector<8x16xf32>, !xegpu.tensor_desc<8x16xf32>
      xegpu.store_nd %out#9, %tileC_0_1 {mode = vc, l1_hint = write_back, l2_hint = write_back, l3_hint = write_back} : vector<8x16xf32>, !xegpu.tensor_desc<8x16xf32>
      xegpu.store_nd %out#10, %tileC_1_0 {mode = vc, l1_hint = write_back, l2_hint = write_back, l3_hint = write_back} : vector<8x16xf32>, !xegpu.tensor_desc<8x16xf32>
      xegpu.store_nd %out#11, %tileC_1_1 {mode = vc, l1_hint = write_back, l2_hint = write_back, l3_hint = write_back} : vector<8x16xf32>, !xegpu.tensor_desc<8x16xf32>

      gpu.return
    }
  }
  memref.global "private" @matA : memref<@MB@x@SIZE@xf16> = dense<1.0>
  memref.global "private" @matB : memref<@SIZE@x@SIZE@xf16> = dense<1.0>
  memref.global "private" @matC : memref<@MB@x@SIZE@xf32> = dense<0.0>
  func.func @main() attributes {llvm.emit_c_interface} {
    %A = memref.get_global @matA : memref<@MB@x@SIZE@xf16>
    %B = memref.get_global @matB : memref<@SIZE@x@SIZE@xf16>
    %C = memref.get_global @matC : memref<@MB@x@SIZE@xf32>
    %2 = call @test(%A, %B, %C) : (memref<@MB@x@SIZE@xf16>, memref<@SIZE@x@SIZE@xf16>, memref<@MB@x@SIZE@xf32>) -> memref<@MB@x@SIZE@xf32>

    return
  }
}
