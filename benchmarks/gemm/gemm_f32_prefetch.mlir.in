// RUN: tpp-run %s -n 10 \
// RUN:  -e entry -entry-point-result=void

// BENCH_TOTAL_FLOPS: 2147483328

// Coarse threading kernel with tuned parameters.
//
// Tile size: 32
// PARAM: map step has to match GEMM tile size
#map = affine_map<(d0) -> (d0 * 32)>

module attributes {gpu.container_module} {
  func.func @entry(%A: memref<@MB@x@SIZE@xf32>, %B: memref<@SIZE@x@SIZE@xf32>, %C: memref<@MB@x@SIZE@xf32>) -> memref<@MB@x@SIZE@xf32> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %A_gpu = gpu.alloc  host_shared () : memref<@MB@x@SIZE@xf32>
    memref.copy %A, %A_gpu : memref<@MB@x@SIZE@xf32> to memref<@MB@x@SIZE@xf32>
    %B_gpu = gpu.alloc  host_shared () : memref<@SIZE@x@SIZE@xf32>
    memref.copy %B, %B_gpu : memref<@SIZE@x@SIZE@xf32> to memref<@SIZE@x@SIZE@xf32>
    %C_gpu = gpu.alloc  host_shared () : memref<@MB@x@SIZE@xf32>
    memref.copy %C, %C_gpu : memref<@MB@x@SIZE@xf32> to memref<@MB@x@SIZE@xf32>

    %dimM = memref.dim %C, %c0 : memref<@MB@x@SIZE@xf32>
    %dimN = memref.dim %C, %c1 : memref<@MB@x@SIZE@xf32>

    %tileSizeM = arith.constant 32 : index
    %tileSizeN = arith.constant 32 : index
    %numTilesX = arith.divui %dimM, %tileSizeM : index
    %numTilesY = arith.divui %dimN, %tileSizeN : index

    // In this example, the matmul tile is <32x32>.
    // However, each thread will compute <4x4> C tile elements.
    // Thus, the block size is reduced to <8x8> threads.
    gpu.launch_func  @entry_kernel::@entry_kernel blocks in (%numTilesX, %numTilesY, %c1) threads in (%c8, %c8, %c1)  args(%A_gpu : memref<@MB@x@SIZE@xf32>, %B_gpu : memref<@SIZE@x@SIZE@xf32>, %C_gpu : memref<@MB@x@SIZE@xf32>, %c0 : index, %c32 : index, %c1 : index)

    // %cast = memref.cast %C_gpu : memref<@MB@x@SIZE@xf32> to memref<*xf32>
    // call @printMemrefF32(%cast) : (memref<*xf32>) -> ()

    gpu.dealloc  %A_gpu : memref<@MB@x@SIZE@xf32>
    gpu.dealloc  %B_gpu : memref<@SIZE@x@SIZE@xf32>

    return %C_gpu : memref<@MB@x@SIZE@xf32>
  }
  func.func private @printMemrefF32(memref<*xf32>) attributes {llvm.emit_c_interface}
  gpu.module @entry_kernel attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.4, [Addresses, Float16Buffer, Int64, Int16, Int8, Kernel, Linkage, Vector16, GenericPointer, Groups, Float16, Float64, AtomicFloat32AddEXT, ExpectAssumeKHR, SubgroupDispatch, VectorComputeINTEL, VectorAnyINTEL], [SPV_EXT_shader_atomic_float_add, SPV_KHR_expect_assume, SPV_INTEL_vector_compute]>, api=OpenCL, #spirv.resource_limits<subgroup_size = 32>>} {
    gpu.func @entry_kernel(%arg0: memref<@MB@x@SIZE@xf32>, %arg1: memref<@SIZE@x@SIZE@xf32>, %arg2: memref<@MB@x@SIZE@xf32>, %arg3: index, %arg4: index, %arg5: index) kernel attributes {VectorComputeFunctionINTEL, spirv.entry_point_abi = #spirv.entry_point_abi<>} {
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c4 = arith.constant 4 : index
      %c8 = arith.constant 8 : index
      %c16 = arith.constant 16 : index
      %c32 = arith.constant 32 : index

      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %4 = affine.apply #map(%0)
      %5 = affine.apply #map(%1)
      %2 = gpu.thread_id  x
      %3 = gpu.thread_id  y // Consecutive increase - vectorizable dim.

      // PARAM: sizes have to match new GEMM tile size
      %subview = memref.subview %arg0[%4, 0] [32, 1024] [1, 1] : memref<@MB@x@SIZE@xf32> to memref<32x1024xf32, strided<[1024, 1], offset: ?>>
      %subview_0 = memref.subview %arg1[0, %5] [1024, 32] [1, 1] : memref<@SIZE@x@SIZE@xf32> to memref<1024x32xf32, strided<[1024, 1], offset: ?>>
      %subview_1 = memref.subview %arg2[%4, %5] [32, 32] [1, 1] : memref<@MB@x@SIZE@xf32> to memref<32x32xf32, strided<[1024, 1], offset: ?>>

      // SMEM sub-tile buffers.
      // PARAM: SMEM caches have to match <(GEMM tile size)x(reduction dim step)>
      %smemA = memref.alloc() : memref<32x16xf32, #spirv.storage_class<Workgroup>>
      %smemB = memref.alloc() : memref<16x32xf32, #spirv.storage_class<Workgroup>>

      // Thread tile sizes.
      // Each thread will compute <4x4> elements of C tile.
      %TM = arith.constant 4 : index
      %TN = arith.constant 4 : index

      // Block tile sizes.
      // Parallel dimensions are based on the original tiling size.
      // Reduction dimension tiling is chosen to match thread tile sizes.
      //
      // PARAM: block sizes BM and BN have to match GEMM tile size
      %BM = memref.dim %subview, %c0 : memref<32x1024xf32, strided<[1024, 1], offset: ?>>
      %BN = memref.dim %subview_0, %c1 : memref<1024x32xf32, strided<[1024, 1], offset: ?>>
      %BK = arith.constant 16 : index

      // Find size of the GEMM tiles reduction dimension.
      // Rectangular sub-tile shape is assumed for simplicity.
      %dimK = memref.dim %subview, %c1 : memref<32x1024xf32, strided<[1024, 1], offset: ?>>
      %numSubTilesK = arith.ceildivsi %dimK, %BK : index

      // Needs constant value for better optimizations.
      // %bDimX = gpu.block_dim x // Threadblock size in X (first) dim.
      // %bDimY = gpu.block_dim y // Threadblock size in Y (second) dim.
      %bDimX = arith.constant 8 : index
      %bDimY = arith.constant 8 : index
      %blockSize = arith.muli %bDimX, %bDimY : index

      // Thread caches.
      // PARAM: sizes have to match TM and TN
      //
      // Accumulate C tile elements in thread registers.
      // Each thread computes <TMxTN> C tile elements.
      %regC = memref.alloca(%TM, %TN) : memref<?x?xf32, #spirv.storage_class<Function>>
      // Thread tile single row (A tile) and col (B tile) register caches.
      %regA = memref.alloca(%TM) : memref<?xf32, #spirv.storage_class<Function>>
      %regB = memref.alloca(%TN) : memref<?xf32, #spirv.storage_class<Function>>

      %threadOffsetRow = arith.muli %2, %TM : index
      %threadOffsetCol = arith.muli %3, %TN : index
      %outTileRow = arith.addi %4, %threadOffsetRow : index
      %outTileCol = arith.addi %5, %threadOffsetCol : index

      %prefetchA_stage_0 = xegpu.create_nd_tdesc %arg0[%outTileRow, %threadOffsetCol] {mode = vc} : memref<@MB@x@SIZE@xf32> -> !xegpu.tensor_desc<4x8xf32>
      %prefetchB_stage_0 = xegpu.create_nd_tdesc %arg1[%threadOffsetRow, %outTileCol] {mode = vc} : memref<@SIZE@x@SIZE@xf32> -> !xegpu.tensor_desc<8x4xf32>
      xegpu.prefetch_nd %prefetchA_stage_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<4x8xf32>
      xegpu.prefetch_nd %prefetchB_stage_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x4xf32>

      // Initialize accumulator registers.
      %threadRow = arith.muli %2, %TM : index
      %threadCol = arith.muli %3, %TN : index
      scf.for %tm = %c0 to %TM step %c1 {
        scf.for %tn = %c0 to %TN step %c1 {
          %cRow = arith.addi %threadRow, %tm : index
          %cCol = arith.addi %threadCol, %tn : index
          // C tile stride 4 elements -> serialized GMEM load [slow].
          // NOTE: These loads can be vectorized to increase performance.
          // NOTE: CUDA compiler might unroll this loop and vectorize the loads.
          %elemC = memref.load %subview_1[%cRow, %cCol] : memref<32x32xf32, strided<[1024, 1], offset: ?>>
          memref.store %elemC, %regC[%tm, %tn] : memref<?x?xf32, #spirv.storage_class<Function>>
        }
      }

      %tRowOffset = arith.muli %2, %bDimX : index
      %tId = arith.addi %tRowOffset, %3 : index

      %tRowA = arith.divui %tId, %BK : index
      %tColA = arith.remui %tId, %BK : index

      %tRowB = arith.divui %tId, %BN : index
      %tColB = arith.remui %tId, %BN : index

      %out:2 = scf.for %subtileIv = %c0 to %numSubTilesK step %c1 iter_args(
        %tA_stage_0 = %prefetchA_stage_0,
        %tB_stage_0 = %prefetchB_stage_0
      ) -> (
        !xegpu.tensor_desc<4x8xf32>, !xegpu.tensor_desc<8x4xf32>
      ) {
        // Load sub-tiles of A and B tiles from GMEM to SMEM.
        // The sub-tiles are loaded cooperatively using all threads in a threadblock.
        // Find the start position of a sub-tile.
        %subtileOffset = arith.muli %subtileIv, %BK : index

        // Fetch data from GMEM to SMEM using all threads in a threadblock.
        // Each thread has to load 2 elements of A and B tiles.
        //
        // PARAM: Step size here should be:
        // numStepsA = (SMEM cache size = BM * BK) / (threadblocksize = 'block dim x' * 'block dim y')
        %smemSizeA = arith.muli %BM, %BK : index
        %numStepsA = arith.divui %smemSizeA, %blockSize : index
        %numElemPerT = arith.constant 1 : index
        %stepSize = arith.muli %blockSize, %numElemPerT : index
        scf.for %subtileStep = %c0 to %numStepsA step %c1 {
          %numLoaded = arith.muli %subtileStep, %stepSize : index
          %stepRowOffset = arith.divui %numLoaded, %BK : index
          // %rowOffset = arith.muli %subtileStep, %stepRowOffset : index
          %rowA = arith.addi %tRowA, %stepRowOffset : index
          %colA = arith.addi %subtileOffset, %tColA : index

          // A tile 4 consecutive elements -> coalesced GMEM load [medium].
          %elemA = memref.load %subview[%rowA, %colA] : memref<32x1024xf32, strided<[1024, 1], offset: ?>>
          memref.store %elemA, %smemA[%rowA, %tColA] : memref<32x16xf32, #spirv.storage_class<Workgroup>>
        }
        // PARAM: Step size here:
        // numStepsB = (SMEM cache size = BN * BK) / (threadblocksize = 'block dim x' * 'block dim y')
        %smemSizeB = arith.muli %BK, %BN : index
        %numStepsB = arith.divui %smemSizeB, %blockSize : index
        scf.for %subtileStep = %c0 to %numStepsB step %c1 {
          %numLoaded = arith.muli %subtileStep, %stepSize : index
          %stepRowOffset = arith.divui %numLoaded, %BN : index
          %tileStart = arith.addi %tRowB, %subtileOffset : index
          %rowB = arith.addi %tileStart, %stepRowOffset : index
          %smemRowB = arith.addi %tRowB, %stepRowOffset : index

          // B tile 32 consecutive elements -> coalesced GMEM load [fast].
          %elemB = memref.load %subview_0[%rowB, %tColB] : memref<1024x32xf32, strided<[1024, 1], offset: ?>>
          memref.store %elemB, %smemB[%smemRowB, %tColB] : memref<16x32xf32, #spirv.storage_class<Workgroup>>
        }

        // Synchronize all threads in a threadblock.
        // Whole A and B sub-tiles are needed to perform computation.
        // Wait for all threads in a threadblock to finish loading A and B tile elements.
        gpu.barrier

        %nextTileA_stage_0 = xegpu.update_nd_offset %tA_stage_0, [%c0, %BK] {mode = vc}: !xegpu.tensor_desc<4x8xf32> -> !xegpu.tensor_desc<4x8xf32>
        %nextTileB_stage_0 = xegpu.update_nd_offset %tB_stage_0, [%BK, %c0] {mode = vc}: !xegpu.tensor_desc<8x4xf32> -> !xegpu.tensor_desc<8x4xf32>

        xegpu.prefetch_nd %nextTileA_stage_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<4x8xf32>
        xegpu.prefetch_nd %nextTileB_stage_0 {mode = vc, l1_hint = cached, l2_hint = cached, l3_hint = cached}: !xegpu.tensor_desc<8x4xf32>

        // GEMM computation.
        %outRowOffset = arith.muli %2, %TM : index
        %outColOffset = arith.muli %3, %TN : index
        scf.for %offset = %c0 to %BK step %c1 {
          scf.for %tm = %c0 to %TM step %c1 {
            %smemRowA = arith.addi %outRowOffset, %tm : index
            %elemA = memref.load %smemA[%smemRowA, %offset] : memref<32x16xf32, #spirv.storage_class<Workgroup>>
            memref.store %elemA, %regA[%tm] : memref<?xf32, #spirv.storage_class<Function>>
          }
          scf.for %tn = %c0 to %TN step %c1 {
            %smemColB = arith.addi %outColOffset, %tn : index

            %cst = arith.constant 0 : index
            %t0 = arith.cmpi eq, %3, %cst : index
            %elemB = memref.load %smemB[%offset, %smemColB] : memref<16x32xf32, #spirv.storage_class<Workgroup>>
            memref.store %elemB, %regB[%tn] : memref<?xf32, #spirv.storage_class<Function>>
          }

          // Outer product on A and B register caches.
          scf.for %tm = %c0 to %TM step %c1 {
            scf.for %tn = %c0 to %TN step %c1 {
              %acc = memref.load %regC[%tm, %tn] : memref<?x?xf32, #spirv.storage_class<Function>>
              %elemA = memref.load %regA[%tm] : memref<?xf32, #spirv.storage_class<Function>>
              %elemB = memref.load %regB[%tn] : memref<?xf32, #spirv.storage_class<Function>>
              %mul = arith.mulf %elemA, %elemB : f32
              %partRes = arith.addf %acc, %mul : f32
              memref.store %partRes, %regC[%tm, %tn] : memref<?x?xf32, #spirv.storage_class<Function>>
            }
          }
        }

        // Synchronize all threads in a threadblock.
        // All current computations have to be finished before SMEM A and B tiles can be
        // replaced with new values (new tiles) from GMEM.
        gpu.barrier

        scf.yield
          %nextTileA_stage_0,
          %nextTileB_stage_0
          : !xegpu.tensor_desc<4x8xf32>, !xegpu.tensor_desc<8x4xf32>
      }

      // Store the final C tile element values.
      scf.for %tm = %c0 to %TM step %c1 {
        scf.for %tn = %c0 to %TN step %c1 {
          %cRow = arith.addi %threadRow, %tm : index
          %cCol = arith.addi %threadCol, %tn : index
          // C tile stride 4 elements -> serialized GMEM store [slow].
          // NOTE: These loads can be vectorized to increase performance.
          // NOTE: CUDA compiler might unroll this loop and vectorize the stores.
          %res = memref.load %regC[%tm, %tn] : memref<?x?xf32, #spirv.storage_class<Function>>
          memref.store %res, %subview_1[%cRow, %cCol] : memref<32x32xf32, strided<[1024, 1], offset: ?>>
        }
      }

      gpu.return
    }
  }
  memref.global "private" @matA : memref<@MB@x@SIZE@xf32> = dense<1.0>
  memref.global "private" @matB : memref<@SIZE@x@SIZE@xf32> = dense<1.0>
  memref.global "private" @matC : memref<@MB@x@SIZE@xf32> = dense<0.0>
  func.func @main() attributes {llvm.emit_c_interface} {
    %A = memref.get_global @matA : memref<@MB@x@SIZE@xf32>
    %B = memref.get_global @matB : memref<@SIZE@x@SIZE@xf32>
    %C = memref.get_global @matC : memref<@MB@x@SIZE@xf32>
    %2 = call @entry(%A, %B, %C) : (memref<@MB@x@SIZE@xf32>, memref<@SIZE@x@SIZE@xf32>, memref<@MB@x@SIZE@xf32>) -> memref<@MB@x@SIZE@xf32>

    return
  }
}
